# ADADELTA: AN ADAPTIVE LEARNING RATE METHOD

[1-1.원문](https://arxiv.org/pdf/1212.5701.pdf)

[1-2.번역](https://translate.google.co.jp/translate?sl=en&tl=ko&js=y&prev=_t&hl=ko&ie=UTF-8&u=https%3A%2F%2Farxiv.org%2Fpdf%2F1212.5701.pdf&edit-text=&authuser=0)


페이지 1
ADADELTA : 적응 학습 속도 방법 Matthew D. Zeiler 1,2 * 1 Google Inc., 미국 2 뉴욕 대학교, 미국 추상 우리는 새로운 차원 당 학습률 방법을 제시한다. 그라데이션 강하 ADADELTA라고. 방법 다이나믹 - 첫 번째 주문 정보 만 사용하여 시간이 지남에 따라 적응합니다. 바닐라 스토아 (vanilla stochas) tic gradient descent. 이 방법은 학습 속도 및 시끄러운 그라디언트 informa - 다양한 모델 아키텍처 선택, 다양한 데이터 모달 - 매개 변수 및 하이퍼 매개 변수 선택. 유망한 것을 보여줍니다. 결과는 MNIST digit clas- 단일 기계를 사용하여 대규모로 분급 작업 수행 분산 된 클러스터 환경에서 음성 데이터 집합. 색인 용어 - 적응 학습 속도, 기계 학습 - 신경 네트워크, 그라디언트 하강 1. 소개 많은 기계 학습 방법의 목적은 목적 함수를 최적화하기 위해 매개 변수 x의 집합 f (x)이다. 이것은 종종 반복적 인 절차를 필요로한다. plies는 매개 변수로 변경되며, Δx는 연산. t 번째 반복에서 매개 변수를 x t 로 표시하면 , 이 간단한 업데이트 규칙은 다음과 같습니다. xt + 1 = xt + Δxt (1) 이 논문에서 우리는 기울기 하강 알고리즘을 고려한다. 다음을 수행하여 목적 함수를 최적화하려고 시도하십시오. 그라데이션의 네가티브에 의해 주어진 가파른 하강 방향 ent g t . 이 일반적인 접근법은 파생물을 얻을 수있는 매개 변수 : Δxt = -ηgt (2) 여기서 g t 는 t 번째 반복에서 매개 변수의 기울기입니다. ∂f (x t ) ∂x t η는 학습 속도로, 음의 기울기의 방향을 취하는 단계. 폴 - 새로운 샘플 또는 배치마다이 음의 기울기를 낮추십시오. 데이터 세트에서 선택한 샘플 중 어느 방향으로 비용이 최소화되는지를 확률 적 구배 강하 (SGD) [ 1 ]. 종종 쉽게 각 파라미터에 대한 기울기를 분석적으로 유도하고, 엔테이션 디센트 알고리즘은 학습 속도 하이퍼 패러미터 - 선택 될 것이다. * 이 작업은 Matthew D. Zeiler가 Google의 인턴이었을 때 수행되었습니다. 학습 속도를 설정하는 것은 전형적으로, 가능한 가장 높은 학습 속도가 선택되는 손으로. 이 비율보다 높게 선택하면 시스템이 발생할 수 있습니다. 목적 함수의 관점에서 분기하고, 이 비율이 너무 낮 으면 학습 속도가 느려집니다. 재화 판별 학습 속도는 많은 사람들에게 과학보다 예술이된다. 문제. 이 작품은 a를 선택하는 작업을 완화하려고 시도 새로운 동적 학습 속도를 도입함으로써 학습 속도를 첫 번째 순서 만 사용하여 차원별로 계산됩니다. 정보. 이것은 사소한 양의 여분의 compu- 그라디언트 디센트 이상의 반복마다. 또한, 이 메소드에는 몇 가지 하이퍼 매개 변수가 있습니다. 그들의 선택은 결과를 크게 바꾸지 않습니다. 혜택 이 접근법은 다음과 같습니다. • 학습 속도의 수동 설정이 없습니다. • 하이퍼 매개 변수에 영향을받지 않습니다. • 차원 당 동적 학습률을 분리합니다. • 기울기 강하에 대한 최소 계산. • 큰 그라데이션, 노이즈 및 아키텍처 선택에 강합니다. • 로컬 또는 분산 환경 모두에 적용 가능합니다. 2. 관련 저작물 그라디언트 디센트 algo- rithm. 가장 강력한 수정은 Newton 's 비용의 2 차 미분을 필요로하는 방법 기능: Δx t = H -1 티 g t (삼) 여기서 H -1 티 Hessian 행렬의 역행렬이다. 반복 t에서 계산 된 미분. 이것은 op- 2 차 문제에 대해 취할 시간 단계 크기, 실제로 대규모 모델을 실제로 계산하는 것은 금지되어 있습니다. 따라서 많은 추가 접근법이 제안되었다 1 차 정보의 사용을 향상 시키거나 1 차 정보의 사용을 개선하거나 제 2 주문 정보에 근접한다. 2.1. 학습률 어닐링 추측을 위해 발견 적 방법을 사용하려는 시도가 여러 차례있었습니다. 그라디언트 디센트 반복마다 좋은 학습 속도를 제공합니다. 이들은 학습이 적절할 때 또는 학습을 시작할 때 학습 속도를 높이려고 시도합니다. 지역 미니 마 근처에서 학습 속도를 늦추십시오. 여기서 우리는 후자의. arXiv : 1212.5701v1 [cs.LG] 2012 년 12 월 22 일
2 쪽
그라디언트 강하가 비용 절감에서 최소치에 근접 할 때, 얼굴, 매개 변수 값을 앞뒤로 진동 수 있습니다 최소치. 이를 막기위한 한 가지 방법은 학습 속도를 감소시켜 매개 변수를 업데이트합니다. 이것은 할 수있다. 유효성 검사의 정확성이 나타날 때 수동으로 수행해야합니다. 고원. 대안으로, 학습률 스케쥴은 프로 (pro- 학습 속도를 자동으로 어닐링하기 위해 [ 1] 제기 데이터를 통해 얼마나 많은 에포크가 완료되었는지. 이 ap- 일반적으로 탐색은 추가 하이퍼 매개 변수를 추가하여 제어합니다. 학습 속도가 얼마나 빨리 감소하는지. 2.2. 차원 당 첫 번째 주문 방법 위에 논의 된 휴리스틱 어닐링 절차는 다음을 수정합니다. 모든 차원에 적용되는 단일 글로벌 학습률 매개 변수 매개 변수 벡터의 각 차원 완전히 다른 방식으로 전체 비용과 관련 될 수 있습니다. 이들을 보완 할 수있는 차원 별 학습률 차이가 종종 유리하다. 2.2.1. 기세 차원 당 교육을 가속화하는 한 가지 방법은 멘텀 방법 [2] . 이것은 아마도 가장 간단한 확장 기능 일 것입니다. SGD는 수십 년 동안 성공적으로 사용되었습니다. 메인 기세 뒤에있는 아이디어는 차원을 넘어서 진보를 가속화하는 것입니다. 그라디언트가 일관되게 같은 방향으로 향하는 시온 기호가있는 치수를 따라 진행 속도를 늦추십시오. 그래디언트의 변화가 계속됩니다. 이것은 기하 급수적 인 감소로 과거 매개 변수 업데이트 추적 : Δx t = ρΔx t-1 - ηg t (4) 여기서 ρ는 이전의 감쇠를 제어하는 ​​상수 매개 변수 업데이트. 이것은 좋은 직관적 인 개선을 제공합니다. 어려운 비용면을 최적화 할 때 SGD를 길고 좁은 계곡. 에도 불구하고 계곡을 따라 그라디언트 계곡을 가로 지르는 구배보다 훨씬 작습니다. 전형적으로 동일한 방향 및 따라서 기세 기간 누적되어 진행 속도를 높입니다. SGD에서는 그라데이션 크기가 작기 때문에 계곡이 느려질 것입니다. 모든 차원에서 공유하는 고정 된 글로벌 학습률 진행 속도를 높일 수 없습니다. 더 높은 학습 속도 선택하기 SGD가 도울 수 있지만 계곡을 가로 지르는 차원은 또한 더 큰 매개 변수 업데이트를 만들어서 산 골짜기를 가로 질러 뒤로 흔들리는 진동. 이들 oscil- 운동량을 사용할 때 기호는 완화되기 때문에 그래디언트 변화와 따라서 운동량 항의 감쇠 이러한 업데이 트를 계곡을 가로 질러 진행 속도를 늦추십시오. 다시, 이것은 차원마다 발생하므로 계곡은 영향을받지 않습니다. 2.2.2. ADAGRAD ADAGRAD [ 3 ] 라고 불리는 최근의 1 차 방법 은 훌륭한 학습 결과를 보여준 우수한 결과 환경에 의존 [ 4 ]. 이 방법은 첫 번째에만 의존합니다. 주문 정보는 있지만 두 번째 주문의 몇 가지 속성이 있습니다. 방법 및 어닐링. ADAGRAD의 업데이트 규칙은 다음과 같이 Δxt = - η √Σ τ = 1 g 2 τ g t (5) 여기서 분모는 모든 이전의 12 번째 놈을 계산합니다 차원 당 기울기 및 η는 글로벌 학습입니다 모든 차원에서 공유되는 비율 손으로 튜닝 한 글로벌 학습 속도가 있지만, 각각 차원에는 자체 동적 속도가 있습니다. 이 동적 속도 그라디언트 크기의 역으로 ​​커지고, dients는 학습률이 작고 작은 그라디언트가 있습니다. 큰 학습 속도. 이것은 좋은 속성을 가지고 있습니다. 각 차원의 진보가 균일 해지는 시간이 지남에 따라 이것은 깊은 neu- 각 레이어의 그라디언트의 스케일은 종종 여러 차수의 주문에 따라 다르므로 최적의 학습 속도는이를 고려해야합니다. 또한,이 분모에서의 그래디언트의 누적은 동일한 ef- 어닐링 (annealing)으로서 fects, 시간이 지남에 따라 학습 속도를 감소 시킴. 그라디언트의 크기가 ADAGRAD,이 방법은 초기 조건에 민감 할 수 있습니다. 매개 변수 및 해당 그라디언트 ini- 평균 기울기가 크면 학습 속도는 훈련의 나머지. 이것은 증가시킴으로써 싸울 수있다. ADAGRAD 방법을 sen- 학습 속도의 선택에 양 감. 또한, 지속적인 분모에 제곱 된 기울기의 누적, 학습률은 교육 기간 동안 계속 감소 할 것이며, 결국 0으로 감소하고 교육을 중지합니다. 놀랍게도. 우리는 ADADELTA 방법을 사용하여 하이퍼 파라미터 선택에 대한 민감성과 피하기 학습 률의 지속적인 감소. 2.3. 두 번째 주문 정보를 사용하는 방법 위의 방법은 그래디언트 및 func- 목표를 최적화하기위한 목표 평가, 두 번째 뉴턴 방정식 또는 준 뉴턴 방정식 메서드는 헤 시안 행렬 또는 근사를 사용합니다. 그것에. 이것은 추가적인 곡률 정보를 제공하지만 최적화에 유용하며, 정확한 2 차 주문 - 형성은 종종 비싸다. 두 번째의 헤 시안 행렬 전체를 계산 한 이후 파생 상품은 대형 모델에 비해 계산 비용이 너무 많이 들지만, Becker와 LecCun 은 대각선 근사를 제안했다. 헤 시안. 이 대각선 근사는 계산 될 수있다. 하나의 추가적인 전방 및 후방 전파 이 모델은 SGD를 능가하는 계산을 효과적으로 두 배로 만듭니다. 헤 시안의 대각선이 계산되면 diag (H), 업데이트 규칙은 다음과 같습니다. Δxt = - 1 | diag (H t ) | + μ g t (6) 이 대각선 헤 시안의 절대 값은 음의 그래디언트 방향이 항상 지켜 지도록하고
Page 3
μ는 작은 상수로 Hes- 작은 곡률의 지역을위한 시안. 최근의 Schaul et al. [ 6] ADAGRAD와 유사한 용어를 사용하는 대각선 헤센 (Hessian) 손으로 특정 학습 속도에 대한 필요성을 완화하기 위해 duced. 이 메서드는 다음 업데이트 규칙을 사용합니다. Δxt = - 1 | diag (H t ) | E [g t-w : t ] 2 E [g 2 t-w : t ] g t (7) 여기서 E [g t-w : t ]는 이전의 w-gram의 기대 값이다. dients 및 E [g 2 t-w : t ]는 제곱 된 gradi- 같은 창문 너머에있는 것 w. Schaul et al. 또한 이 창 크기에 대한 휴리스틱 (자세한 내용은 [ 6] 참조). 3. ADADELTA 방법 이 논문에서 제시된 아이디어는 ADA- GRAD [3 ] 는 두 가지 주요 무승부를 개선하기 위해, 방법의 뒷받침 : 1) 학습 률의 지속적인 감소 2) 수동으로 선택해야하는 필요성 글로벌 학습 속도. 우리가 발견 한 방법을 파생시킨 후에 Schaul et al. [ 6] 아래로 껍질을 벗기다. ADAGRAD 방법에서 분모는 누적된다. 각 반복에서 구배 - 훈련의 시작. 각 학기가 양수이기 때문에, 총액은 교육 기간 내내 지속적으로 증가합니다. 각 차원에서 학습 속도를 줄입니다. 많은 it- 이 학습 속도는 극도로 작아 질 것입니다. 3.1. 아이디어 1 : 누적 창 모두에 대한 제곱 된 그래디언트의 합계를 축적하는 대신 우리는 과거의 그라디언트 창을 제한했다. 일부 고정 크기 w가 누적됩니다 (크기 t 대신 t는 ADAGRAD에서와 같이 현재 반복이다). 이 윈 - ADAGRAD의 분모는 무한대로 누적되어 대신 지역 견적이된다. 최근의 그라디언트를 사용합니다. 이것은 학습이 계속되는 것을 보장합니다. 많은 업데이트 반복 후에도 진행을 이미 함. 이전의 제곱 된 기울기를 저장하는 것은 비효율적이므로, 우리의 방법은이 축적을 exponen- 제곱 된 그라디언트의 평균 감소. 가정 시간 t이 이동 평균은 E [g 2 ] t 이어서 우리는 다음을 계산합니다 : E [ g2] t = ρE [g2] t-1 + (1 - ρ) g2 티 (8) 여기서 ρ는 모멘텀 (momen- 텀 방법. 이 양의 제곱근이 필요하기 때문에 매개 변수 업데이트에서 이것은 효과적으로 RMS가됩니다 이전의 제곱 된 그라디언트의 최대 시간 t : RMS [g] t = √E [g 2 ] t + ε (9) 여기서 denomina- tor [ 5] 에서와 같이 . 결과 매개 변수 업데이트는 다음과 같습니다. Δxt = - η RMS [g] t g t (10) 알고리즘 1 시간 t에서 ADADELTA 업데이트 요구 : 감쇠율 ρ, 상수 ε 필요 : 초기 매개 변수 x 1 1 : 누적 변수 초기화 E [g 2 ] 0 = 0, E [Δx 2 ] 0 = 0 2 : t = 1 일 경우 : # 회 이상 업데이트 삼: 계산 기울기 : g t 4 : Gradient 누적 : E [g 2 ] t = ρE [g 2 ] t-1 + (1 - ρ) g 2 티 5 : 계산 업데이트 : Δx t = - RMS [Δx] t-1 RMS [g] t g t 6 : 누적 업데이트 : E [ Δx2] t = ρE [Δx2] t-1 + (1-ρ) Δx2 티 7 : 업데이트 적용 : x t + 1 = x t + Δ x t 8 : 끝 3.2. 아이디어 2 : 헤 시안 근사법으로 정확한 단위 매개 변수 업데이트를 고려할 때 Δx가 적용됩니다. x에 일치하면 일치해야합니다. 즉, 매개 변수에 일부 가상 단위, 매개 변수의 변경 사항은 그 단위들에서도 변화가있을 것입니다. SGD를 고려할 때, 모멘텀, 또는 ADAGRAD, 우리는 이것이 케이스. SGD 및 모멘텀의 단위는 그라디언트와 관련이 있으며, 매개 변수가 아닙니다. g 단위의 Δx α 단위 ∂f ∂x α 1 단위 x (11) 비용 함수 f를 단위가 없다고 가정 할 때. ADAGRAD도 업데이트에 비율이 포함되어 있으므로 올바른 단위가 없습니다. 그래디언트 양, 따라서 단위가없는 업데이트입니다. 대조적으로, Newton 's 헤 시안 정보 또는 근사법을 사용하는 방법 Hessian에게는 매개 변수에 대한 정확한 단위가 있습니다. 업데이트 : Δx α H -1 g α ∂f ∂x ∂2f ∂x 2 α 단위의 x (12) 용어의 사용을 고려한 단위의 불일치를 Eqn에 더한다. 10 업데이트 단위가 일치하도록 매개 변수의 단위. 2 차 방법은 올바른, 우리는 뉴턴의 방법을 재 배열한다 (대각선 가정 Hessian)을 결정하기 위해 2 차 미분의 역수를 구한다. 관련된 양 : Δx = ∂f ∂x ∂2f ∂x 2 ⇒ 1 ∂2f ∂x 2 = Δx ∂f ∂x (13) 이전 그라디언트의 RMS가 이미 표현 되었기 때문에, Eqn.의 분모로 보내진다. 10 우리는 mea- 분자의 Δx 양을 확인하십시오. 현재의 Δx t 시간 단계는 알려지지 않았으므로 곡률은 국부적으로 가정합니다 기하 급수적으로 계산하여 Δx를 부드럽게하고 근사화 하십시오. 이전 Δx의 크기 w의 창에 대해 RMS를 감쇠시키는 ADADELTA 방법을 제공하십시오 : Δxt = - RMS [Δx] t-1 RMS [g] t g t (14) 동일한 상수 ε가 분자 RMS에 다음과 같이 더해진다. 잘. 이 상수는 첫 번째부터 시작하여
Page 4
Δx 0 = 0 인 반복 및 진행을 보장하기 위해 이전 업데이트가 작아 지더라도 만들어 질 수 있습니다. 이 유도는 대각선 curva- 두 번째 파생 상품이 쉽게 재 배열 될 수 있도록해야합니다. 또한, 이것은 대각선 헤 시안에 대한 근사치입니다 g 및 Δx의 RMS 측정 만 사용하십시오. 이 근사치 Becker and LeCun [ 5] 에서 와 같이 항상 긍정적이다 . 업데이트 방향은 각 단계에서 음의 기울기를 따릅니다. 식 도 14에서 , RMS [Δx] t-1 양은 디 - 지명 된 사람은 재발 관계 때문에 1 시간 단계 씩 Δx t 동안 . 이것의 흥미로운 부작용은 시스템이 큰 갑작스러운 그라디언트에 강건하기 때문에 디 - 추천인, 현행 학습 효과율 감소 분자가 반응 할 수 있기 전에 시간 단계. 식 14 1 차 정보 만 사용합니다. 논의 된 각각의 메티오닌으로부터의 몇몇 특성을 가지며, ods. 현재 반복에 대한 음의 그래디언트 방향입니다. -g t 는 SGD에서와 같이 항상 따라옵니다. 분자는 다음과 같이 동작합니다. 가속 기간, 이전의 그라디언트를 운동량과 마찬가지로 시간의 창. 분모는 다시 ADAGRAD에 제곱 된 기울기 정보 디멘션별로 각 di- mension)이지만, 진도를 보장하기 위해 창을 통해 계산됩니다 나중에 교육을받습니다. 마지막으로,이 방법은 Schaul et al. 헤 시안 (Hessian)에 대한 근사가 만들어 졌기 때문에, 대신 반복 당 하나의 그래디언트 계산에 비용이 소요됩니다. 과거의 업데이트 정보를 활용합니다. com- 알고리즘 세부 정보는 알고리즘 1을 참조하십시오 . 4. 실험 우리는 여러 다른 방법을 사용하여 두 가지 작업에 대한 우리의 방법을 평가합니다. 신경 네트워크 아키텍처. 우리는 신경망을 훈련시킨다. SGD, 모멘텀, ADAGRAD 및 ADADELTA를 사용하여 교차 엔트로피 목표를 최소화하기위한 감독 된 방식 네트워크 출력과 접지 사실 레이블 사이. 비교 - 등가물은 로컬 컴퓨터와 분산 된 컴퓨터에서 모두 수행됩니다. 클러스터를 계산하십시오. 4.1. 자필 자리 분류 실험의 첫 번째 세트에서 우리는 신경망을 훈련시킵니다. MNIST 손으로 쓰는 자리 분류 작업. 비교하려고 Schaul et al. 우리가 tanh nonlinear- 첫 번째 레이어에는 500 개의 숨겨진 유닛이 있고 그 뒤에 300 개가옵니다. 숨겨진 유닛을 두 번째 레이어에, 마지막 softmax 아웃 - 상단에 레이어를 넣어. 우리의 방법은 교육 세트를 통해 6 개 에포크 당 배치 당 100 개의 이미지. 하이퍼 파라미터를 ε = 1e-6 및 ρ = 0.95로 설정하면 Schaul의 2.10 %에 비해 2.00 %의 테스트 세트 오류를 ​​달성합니다. et al. 이것은 거의 컨버전스가되지 않지만 감각을 제공합니다. 알고리즘이 얼마나 빨리 분류를 최적화 할 수 있는지 목표. 0 10 20 30 40 50 개 1 1.5 2 2.5 삼 3.5 4 4.5 5 5.5 6 시대 테스트 오류 % SGD 기세 ADAGRAD ADADELTA 그림 1. MNIST 숫자에 대한 학습률 방법의 비교 50 에포크 분류. 다양한 수렴 방법을 더 자세히 분석하기 위해 처음에는 500 개의 숨겨진 유닛으로 동일한 신경 네트워크를 훈련하십시오. 계층, 두 번째 계층에서 300 개의 숨겨진 유닛 및 정류 된 선형 50 epoch에 대해 두 레이어에서 활성화 기능을 수행합니다. 우리가 알아 차리는 직선형 선형 단위는 실제로 tanh보다 잘 작동합니다. 그들의 포화되지 않는 특성은 각 방법을 더 테스트합니다. 활성화 및 그라디언트의 큰 변화에 대처할 때. Fig . 1 SGD, 모멘텀, ADAGRAD, ADADELTA는 테스트 세트 오류를 ​​최적화합니다. unal- 이 경우 SGD 방식이 최악이되는 반면, 그것에 대한 모멘텀은 성능을 크게 향상시킵니다. ADAGRAD는 훈련의 처음 10 개 에포크 (epoch) 그 이후에 그것은 디 - 계속 증가하는 지명자. ADADELTA 경기 계속하는 동안 ADAGRAD의 빠른 초기 수렴 테스트 오류를 ​​줄이기 위해 최상의 성능에 가까운 수렴 운동량과 함께 발생합니다. SGD 기세 ADAGRAD ε = 1e 0 2.26 % 89.68 % 43.76 % ε = 1e -1 2.51 % 2.03 % 2.82 % ε = 1e- 2 7.02 % 2.68 % 1.79 % ε = 1e -3 17.01 % 6.98 % 5.21 % ε = 1e- 4 58.10 % 16.98 % 12.59 % 표 1. 교육의 6 개 에포크 이후의 MNIST 테스트 오류율 SGD, MOMENTUM 등을 사용하는 다양한 하이퍼 매개 변수 설정 및 ADAGRAD. ρ = 0.9 ρ = 0.95 ρ = 0.99 ε = 1e- 2 2.59 % 2.58 % 2.32 % ε = 1e- 4 2.05 % 1.99 % 2.28 % ε = 1e -6 1.90 % 1.83 % 2.05 % ε = 1e- 8 2.29 % 2.13 % 2.00 % 표 2. 다양성에 대한 6 에포크 이후의 MNIST 테스트 오류율 ADADELTA를 사용한 하이퍼 매개 변수 설정.
Page 5
4.2. 하이퍼 매개 변수에 대한 민감도 기세가 더 나은 최종 해결책으로 수렴되는 동안 ADADELTA는 많은 신기원 훈련을 거친 후, SGD와 ADA- 그레이드. 표 1에서 우리 는 각 방법의 학습 속도를 다양하게합니다 다음을 사용하여 6 개 신기원 이후의 테스트 세트 오류를 ​​보여줍니다. 정류 된 선형 유닛을 활성화 기능으로 사용합니다. 최적의 각 열의 설정을 사용하여 그림 1 을 생성했습니다 . 와 SGD, Momentum 또는 ADAGRAD 학습 속도가 필요합니다. 정확한 크기의 순서로 설정되어야하는데, 평균 속도는 일반적으로 발산하고 그 이하에서는 최적화 천천히 진행됩니다. 우리는 이러한 결과가 매우 다양하다는 것을 알 수 있습니다. 표 2의 ADADELTA와 비교하여 각 방법에 대해 가능한 2 개의 하이퍼 파라미터는 크게 변경되지 않는다. 공연. 4.3. 효과적인 학습 속도 ADADELTA의 일부 속성을 조사하기 위해 우리는 음모를 꾸미고 그림 2 에서 무작위로 10의 스텝 사이즈와 파라미터 업데이트 3 개의 가중치 행렬들 각각에서 선택된 치수들, 밖으로 훈련. 몇 가지 흥미로운 점이 분명합니다. 이 그림. 첫째, 단계 크기, 또는 효과적인 학습 속도 (모두 식에서 g t 를 제외한 항 . 도 14 ) 는 그림은 네트워크의 하위 계층에서 더 크고 트레이닝 시작시 맨 위 레이어의 크기가 작습니다. 이 ADADELTA의 재산은 층은 감소하는 그라디언트로 인해보다 작은 그라디언트를 갖는다. 0 100 200 0 0.5 1 η × 1 0 100 200 0 0.5 1 η × 2 0 100 200 0 0.5 1 η × 3 0 100 200 -0.01 0 0.01 A × 1 0 100 200 -0.01 0 0.01 A × 2 0 100 200 -0.01 0 0.01 A × 3 그림 2. 단계 크기 및 매개 변수 업데이트 tanh non- 25 에포크에 대한 선형성. 왼쪽 : 무작위로 10 단계 크기 각각의 3 개의 가중치 행렬의 선택된 치수 회로망. 오른쪽 : 동일한 10 차원의 매개 변수가 변경됩니다. 3 개의 가중치 행렬 각각에 대한 값. 큰 단계에 주목하십시오. 하위 레이어의 크기로 인해 사라지는 그레이 스케일을 보완하는 데 도움이됩니다. backpropagation으로 발생하는 dients. 신경망에서 문제가 발생하므로 더 커야한다. 학습 속도. 둘째, 훈련이 끝날 무렵, 1로 끝내십시오. 이것은 일반적으로 높은 학습 속도입니다. 대부분의 방법에서 발산을 유도하지만, 훈련이 끝날 무렵 1시에만 그라디언트 및 매개 변수 업데이트가 적습니다. 이 시나리오에서, 분자와 분모의 ε 상수는 지배적이다. 과거의 그라디언트 및 매개 변수 업데이트, 1의 학습 률. 이것은 ADADELTA의 마지막 흥미로운 자산으로 연결됩니다. 스텝 사이즈가 1이되면, 파라미터 업데이트 (도 2 의 우측에 도시 됨 )는 0으로 향하는 경향이있다. 이 각 가중치 행렬에 대해 효과적으로 발생 함 어닐링 일정이있는 것처럼 작동합니다. 그러나 명시적인 어닐링 일정이 없으면 학습 속도에 대한 이유는 적절한 하이퍼 파라미터는 나중에 ADADELTA보다 뛰어난 성능을 보입니다. 도 1 에서 볼 수있다 . 운동량으로 인해 발생할 수있는 진동 최소치 부근에서는 부드럽게 나오고 ADADELTA 이것들은 분자에 누적 될 수 있습니다. 어닐링 (annealing) ule을 ADADELTA 메소드에 추가하여 향후 작업에서이를 막아줍니다. 4.4. 음성 데이터 다음 실험 세트에서 우리는 대규모 neu- 몇 백 시간 동안 4 개의 숨겨진 레이어가있는 네트워크 음성 검색, 음성 IME, 데이터를 읽습니다. 네트워크는 분산 된 시스템은 [ 4] 의 중앙 집중 형 매개 변수 서버 accu- 여러 개에서 다시보고 된 그라디언트 정보를 mulates합니다. 신경 네트워크의 복제본. 우리의 실험에서 우리는 ei- ther 100 또는 200과 같은 성능을 테스트하는 복제 네트워크 고도로 분산 된 환경에서 ADADELTA의 뉴럴 네트워크는 [ 7 ]에서 와 같이 셋업된다. 오디오의 26 프레임이며 각각 40 로그 에너지 필터로 구성됩니다. ter 은행 출력. 네트워크 출력은 8,000 GMM-HMM 시스템에서 생산 된 senone 레이블 입력 프레임과의 강제 정렬 각 숨겨진 레이어 신경 네트워크의 2560 개의 숨겨진 유닛이 있고 훈련을 받았습니다. 물류 또는 정류 된 선형 비선형 성을 갖는다. 무화과. 3은 ADADELTA 방법의 성능을 보여준다. 100 개의 네트워크 복제본을 사용할 때. 우리의 방법 ini- ADAGRAD through- 프레임 분류 정확도 측면에서 테스트 세트. ε = 1e -6 과 ρ = 0.95 의 동일한 설정은 MNIST 실험은이 설정에 사용되었습니다. 정류 된 선형 유니트로 교육하고 200 모델 복제본에서 우리는 같은 설정의 hyperpa- (그림 4 참조 ). 200 개의 복제물을 가지고 있음에도 불구하고 본질적으로 의미있는 양의 잡음을 gra- ADADELTA 방법은 잘 수행되지만, 다른 프레임과 동일한 프레임 정확도로 빠르게 수렴 행동 양식.
Page 6
0 20 40 60 80 100 18 20 22 24 26 세 28 30 32 34 36 38 세 시간 (시간) 명성 Acc % uracy ADAGRAD 로그 ADADELTA 로그 그림 3. ADAGRAD와 ADADELTA의 비교 물류 비선형 성을 이용한 100 개의 복제본을 사용한 음성 데이터 세트. 0 20 40 60 80 100 120 140 160 15 명 20 25 명 30 35 세 시간 (시간) 프레임 정확도 % ADAGRAD relu ADADELTA relu 모멘텀 relu 그림 4. ADAGRAD, Momentum 및 ADADELTA는 200 개의 복제본을 사용하여 음성 데이터 세트를 사용합니다. 정류 된 선형 비선형 성. 5. 결론 이 기술 보고서에서는 새로운 학습 속도 방법을 도입했습니다. promis-1을 나타내는 1 차 정보에만 기초하여, MNIST에서의 결과와 대규모 음성 인식 데이터 세트. 이 방법은 사소한 계산 오버 헤드를 가지고있다. 차원 당 학습률을 제공하면서 SGD와 비교합니다. 입력 데이터 유형의 폭 넓은 변형에도 불구하고, hid- den 단위, 비선형 성 및 분산 복제본 수, 하이퍼 파라미터는 튜닝 할 필요가 없었습니다. ADADELTA는 강력한 학습 속도 방법으로, 다양한 상황에 묶여있다. 감사의 말씀 우리는 요람 (Georin Hinton), 요람 (Yoram) 가수, Ke Yang, Marc'Aurelio Ranzato 및 Jeff Dean 이 작품에 관한 유용한 의견과 토론. 6. 참고 문헌 [1] H. Robinds와 S. Monro, "확률 론적 근사 방법, "수학 통계의 실록, vol. 22, pp. 400-407, 1951. [2] DE Rumelhart, GE Hinton, RJ Williams, "Learn- 역 전파 오류에 의한 표현 표현 "Nature, vol. 323, pp. 533-536, 1986]에 기재되어있다. [3] J. Duchi, E. Hazan 및 Y. Singer, "적응 하위 구배 온라인 기울이기 및 확률 적 최적화 방법 " 2010 년 COLT. [4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng, "대규모 분산 된 깊은 네트 - 작품 ", NIPS, 2012. [5] S. Becker와 Y. LeCun, "수렴 개선 2 차 방법으로 역 전파 학습 " Tech. 하원 대학교 컴퓨터 과학과 토론토, 토론토, ON, 캐나다, 1988 년 [6] T. Schaul, S. Zhang, and Y. LeCun, "더 성가신 일 없음 학습 속도 "(arXiv : 1206.1106, 2012). [7] N. Jaitly, P. Nguyen, A. Senior 및 V. Vanhoucke, "Ap- pretrained 깊은 신경 네트워크의 큰 vo - cabulary speech recognition ", 2012 년 Interspeech에서 발표했습니다.
